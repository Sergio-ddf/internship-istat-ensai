{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning and formatting"
      ],
      "metadata": {
        "id": "PtNaQ40Nltxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5379-CEaloTD",
        "outputId": "9e7c8705-988d-4efe-da6f-9cb0441480bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repair completed: {'total_records': 218506, 'repaired_with_extra_semicolons': 21889, 'irreparable': 0}\n",
            "Repaired file written to: /content/dataset_gbv_repaired.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import unicodedata\n",
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "INPUT_CSV   = \"/content/gbv_2023Q4.csv\"\n",
        "REPAIRED_CSV = \"/content/dataset_gbv_repaired.csv\"\n",
        "CLEAN_CSV    = \"/content/dataset_gbv_clean.csv\"\n",
        "\n",
        "# Allowed values\n",
        "ALLOWED_SENTIMENT = {\"POS\", \"NEG\", \"-\"}\n",
        "ALLOWED_INFO      = {\"INFO\", \"NOINFO\", \"-\"}\n",
        "\n",
        "# Allowed emotions\n",
        "ALLOWED_EMOTIONS = {\n",
        "    \"GIOIA\",\"TRISTEZZA\",\"RABBIA\",\"PAURA\",\"DISGUSTO\",\"SORPRESA\",\"FIDUCIA\",\"ATTESA\",\"NEUTRA\"\n",
        "}\n",
        "\n",
        "# Minimal patterns and sets\n",
        "ISO_Z_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$\")\n",
        "INFO_SET = ALLOWED_INFO\n",
        "\n",
        "# Row repair: recompose TEXT and realign last 4 columns\n",
        "# Output columns: [ID, DATE, CHANNEL, TEXT, SENTIMENT, EMOTION, INFO, URI]\n",
        "\n",
        "def looks_like_record(tokens):\n",
        "    \"\"\"Heuristic: last-2 must be allowed INFO; col 2 is ISO-Z date; plausible ID length.\"\"\"\n",
        "    if len(tokens) < 8:\n",
        "        return False\n",
        "    info = tokens[-2].strip()\n",
        "    if info not in INFO_SET:\n",
        "        return False\n",
        "    if ISO_Z_PATTERN.match(tokens[1].strip()) is None:\n",
        "        return False\n",
        "    if len(tokens[0].strip()) < 16:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def csv_repair(in_path, out_path, encoding=\"utf-8\"):\n",
        "    fixed = total = bad = 0\n",
        "    header_written = False\n",
        "\n",
        "    with open(in_path, \"r\", encoding=encoding, errors=\"replace\") as fin, \\\n",
        "         open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
        "\n",
        "        writer = csv.writer(fout, delimiter=\";\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        buffer = \"\"\n",
        "        header = None\n",
        "\n",
        "        for raw in fin:\n",
        "            line = raw.rstrip(\"\\n\")\n",
        "            if not line.strip() and not buffer:\n",
        "                continue\n",
        "\n",
        "            if not header_written:\n",
        "                header = line\n",
        "                writer.writerow([\"ID\",\"DATE\",\"CHANNEL\",\"TEXT\",\"SENTIMENT\",\"EMOTION\",\"INFO\",\"URI\"])\n",
        "                header_written = True\n",
        "                continue\n",
        "\n",
        "            buffer = (buffer + \"\\n\" + line) if buffer else line\n",
        "            tokens = [t for t in buffer.split(\";\")]\n",
        "\n",
        "            if not looks_like_record(tokens):\n",
        "                continue\n",
        "\n",
        "            total += 1\n",
        "            try:\n",
        "                _id       = tokens[0].strip()\n",
        "                _date     = tokens[1].strip()\n",
        "                _channel  = tokens[2].strip()\n",
        "                _sent     = tokens[-4].strip()\n",
        "                _emo      = tokens[-3].strip()\n",
        "                _info     = tokens[-2].strip()\n",
        "                _uri      = \";\".join(tokens[-1:]).strip()\n",
        "                _text     = \";\".join(tokens[3:-4]).strip().replace('\"', '\"\"')\n",
        "\n",
        "                writer.writerow([_id, _date, _channel, _text, _sent, _emo, _info, _uri])\n",
        "                if len(tokens) > 8:\n",
        "                    fixed += 1\n",
        "\n",
        "            except Exception:\n",
        "                bad += 1\n",
        "            finally:\n",
        "                buffer = \"\"\n",
        "\n",
        "        if buffer.strip():\n",
        "            bad += 1\n",
        "\n",
        "    return dict(total_records=total, repaired_with_extra_semicolons=fixed, irreparable=bad)\n",
        "\n",
        "stats = csv_repair(INPUT_CSV, REPAIRED_CSV)\n",
        "print(\"Repair completed:\", stats)\n",
        "print(\"Repaired file written to:\", REPAIRED_CSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load, clean allowed values, and apply filters"
      ],
      "metadata": {
        "id": "r-ZjR6vymSxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(REPAIRED_CSV, sep=\";\", dtype=str, keep_default_na=False, na_values=[])\n",
        "\n",
        "for c in [\"SENTIMENT\",\"EMOTION\",\"INFO\",\"CHANNEL\"]:\n",
        "    if c in df:\n",
        "        df[c] = df[c].astype(str).str.strip().str.upper()\n",
        "\n",
        "mask_sent_bad = ~df[\"SENTIMENT\"].isin(ALLOWED_SENTIMENT)\n",
        "df.loc[mask_sent_bad, \"SENTIMENT\"] = \"-\"\n",
        "\n",
        "mask_emo_bad = ~df[\"EMOTION\"].isin(ALLOWED_EMOTIONS)\n",
        "df.loc[mask_emo_bad, \"EMOTION\"] = pd.NA\n",
        "\n",
        "mask_keep_info = (df[\"INFO\"] == \"INFO\")\n",
        "\n",
        "before = len(df)\n",
        "df = df[mask_keep_info].copy()\n",
        "df = df[df[\"EMOTION\"].notna()].copy()\n",
        "after = len(df)\n",
        "print(f\"Rows before: {before} | after filters: {after} | removed: {before - after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMtstXHymFkW",
        "outputId": "d0572c36-317b-42fa-a701-bcaeb260d42a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows before: 218506 | after filters: 71609 | removed: 146897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity checks + final save"
      ],
      "metadata": {
        "id": "mYJGMGuzmfCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected_cols = [\"ID\",\"DATE\",\"CHANNEL\",\"TEXT\",\"SENTIMENT\",\"EMOTION\",\"INFO\",\"URI\"]\n",
        "assert list(df.columns) == expected_cols, f\"Unexpected columns: {df.columns.tolist()}\"\n",
        "assert df[\"DATE\"].str.match(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$\").all(), \"DATE not ISO-Z on some rows\"\n",
        "assert df[\"INFO\"].eq(\"INFO\").all(), \"INFO filter failed\"\n",
        "assert df[\"SENTIMENT\"].isin(ALLOWED_SENTIMENT).all(), \"Invalid SENTIMENT values\"\n",
        "assert df[\"EMOTION\"].isin(ALLOWED_EMOTIONS).all(), \"Invalid EMOTION values\"\n",
        "\n",
        "print(\"Final shape (rows, cols):\", df.shape)\n",
        "df.to_csv(CLEAN_CSV, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "print(\"Clean file written to:\", CLEAN_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAERi2J-mbQ2",
        "outputId": "517408e5-c2d1-42e9-ac8f-be605c3a2f06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final shape (rows, cols): (71609, 8)\n",
            "Clean file written to: /content/dataset_gbv_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of clean file"
      ],
      "metadata": {
        "id": "PfEdkWHKmnr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLEAN_CSV   = \"/content/dataset_gbv_clean.csv\"\n",
        "OUT_REPORT  = \"/content/gbv_counts_summary.csv\"\n",
        "ALLOWED_INFO_LIST      = [\"INFO\", \"NOINFO\", \"-\"]\n",
        "ALLOWED_SENTIMENT_LIST = [\"POS\", \"NEG\", \"-\"]\n",
        "ALLOWED_EMOTIONS_LIST  = [\"GIOIA\",\"TRISTEZZA\",\"RABBIA\",\"PAURA\",\"DISGUSTO\",\"SORPRESA\",\"FIDUCIA\",\"ATTESA\",\"NEUTRA\"]\n",
        "\n",
        "def analyze_clean_file(path=CLEAN_CSV, out_report=OUT_REPORT):\n",
        "    df = pd.read_csv(path, sep=\";\", dtype=str, keep_default_na=False, na_values=[])\n",
        "\n",
        "    expected = [\"ID\",\"DATE\",\"CHANNEL\",\"TEXT\",\"SENTIMENT\",\"EMOTION\",\"INFO\",\"URI\"]\n",
        "    missing = [c for c in expected if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in clean CSV: {missing}\")\n",
        "\n",
        "    for c in [\"INFO\",\"SENTIMENT\",\"EMOTION\"]:\n",
        "        df[c] = df[c].astype(str).str.strip().str.upper()\n",
        "\n",
        "    total_rows = len(df)\n",
        "    info_counts = df[\"INFO\"].value_counts().reindex(ALLOWED_INFO_LIST, fill_value=0)\n",
        "    sent_counts = df[\"SENTIMENT\"].value_counts().reindex(ALLOWED_SENTIMENT_LIST, fill_value=0)\n",
        "    emo_counts  = df[\"EMOTION\"].value_counts().reindex(ALLOWED_EMOTIONS_LIST, fill_value=0)\n",
        "\n",
        "    print(f\"Total rows: {total_rows}\\n\")\n",
        "    print(\"Counts by INFO (INFO/NOINFO/-):\")\n",
        "    print(info_counts.to_string(), \"\\n\")\n",
        "    print(\"Counts by SENTIMENT (POS/NEG/-):\")\n",
        "    print(sent_counts.to_string(), \"\\n\")\n",
        "    print(\"Counts by EMOTION:\")\n",
        "    print(emo_counts.to_string(), \"\\n\")\n",
        "\n",
        "    report_rows = []\n",
        "    for label, cnt in info_counts.items():\n",
        "        report_rows.append({\"group\":\"INFO\", \"label\":label, \"count\":int(cnt)})\n",
        "    for label, cnt in sent_counts.items():\n",
        "        report_rows.append({\"group\":\"SENTIMENT\", \"label\":label, \"count\":int(cnt)})\n",
        "    for label, cnt in emo_counts.items():\n",
        "        report_rows.append({\"group\":\"EMOTION\", \"label\":label, \"count\":int(cnt)})\n",
        "\n",
        "    report_df = pd.DataFrame(report_rows, columns=[\"group\",\"label\",\"count\"])\n",
        "    report_df.to_csv(out_report, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "    print(f\"Report saved to: {out_report}\")\n",
        "\n",
        "    return {\n",
        "        \"total_rows\": total_rows,\n",
        "        \"info_counts\": info_counts,\n",
        "        \"sent_counts\": sent_counts,\n",
        "        \"emo_counts\": emo_counts,\n",
        "        \"report_df\": report_df,\n",
        "    }\n",
        "\n",
        "_ = analyze_clean_file()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaEdy-cFmitF",
        "outputId": "c5e661ec-405a-4503-f702-5310f1077b9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 71609\n",
            "\n",
            "Counts by INFO (INFO/NOINFO/-):\n",
            "INFO\n",
            "INFO      71609\n",
            "NOINFO        0\n",
            "-             0 \n",
            "\n",
            "Counts by SENTIMENT (POS/NEG/-):\n",
            "SENTIMENT\n",
            "POS    37424\n",
            "NEG    13479\n",
            "-      20706 \n",
            "\n",
            "Counts by EMOTION:\n",
            "EMOTION\n",
            "GIOIA        14967\n",
            "TRISTEZZA    12442\n",
            "RABBIA       14027\n",
            "PAURA          568\n",
            "DISGUSTO         0\n",
            "SORPRESA      1394\n",
            "FIDUCIA          0\n",
            "ATTESA           0\n",
            "NEUTRA       28211 \n",
            "\n",
            "Report saved to: /content/gbv_counts_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deduplicate TEXT + post-dedup analysis"
      ],
      "metadata": {
        "id": "QV_TVCDEmyBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Remove Twitter prefixes from TEXT\n",
        "# Pattern: <something>@twitter.com (User Name) [RT ]@<handle>...\n",
        "# Output: cleaned dataset + audit with original and cleaned text\n",
        "# =========================================================\n",
        "IN_CSV    = \"/content/dataset_gbv_clean.csv\"\n",
        "OUT_CSV   = \"/content/dataset_gbv_clean2.csv\"\n",
        "AUDIT_CSV = \"/content/gbv_twclean_audit.csv\"\n",
        "\n",
        "def strip_twitter_prefix(text: str):\n",
        "    \"\"\"\n",
        "    Remove leading\n",
        "      <something>@twitter.com (User Name) [RT ]@<handle>...\n",
        "    Returns (new_text, was_modified)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text, False\n",
        "\n",
        "    pattern = re.compile(\n",
        "        r\"^.+?@twitter\\.com\\s*\\([^)]*\\)\\s*(?:RT\\s+)?(?:@\\w+\\s*)+\",\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    new_text, n = pattern.subn(\"\", text, count=1)\n",
        "    return new_text.strip(), n > 0\n",
        "\n",
        "# 1) Load\n",
        "df = pd.read_csv(IN_CSV, sep=\";\", dtype=str, keep_default_na=False, na_values=[])\n",
        "\n",
        "# 2) Strip prefixes\n",
        "mask = df[\"CHANNEL\"].astype(str).str.upper().eq(\"TWITTER\")\n",
        "subset = df.loc[mask].copy()\n",
        "subset[\"TEXT_ORIG\"] = subset[\"TEXT\"]\n",
        "subset[\"TEXT_STRIPPED\"], subset[\"_TW_PREFIX_REMOVED\"] = zip(*subset[\"TEXT\"].map(strip_twitter_prefix))\n",
        "df.loc[subset.index, \"TEXT\"] = subset[\"TEXT_STRIPPED\"]\n",
        "\n",
        "# 3) Audit\n",
        "audit = subset.loc[subset[\"_TW_PREFIX_REMOVED\"], [\"ID\",\"DATE\",\"CHANNEL\",\"TEXT_ORIG\",\"TEXT_STRIPPED\"]].copy()\n",
        "audit.rename(columns={\"TEXT_STRIPPED\": \"TEXT_NEW\"}, inplace=True)\n",
        "\n",
        "# 4) Save\n",
        "df.to_csv(OUT_CSV, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "audit.to_csv(AUDIT_CSV, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "print(f\"Cleaning completed. Modified rows: {len(audit)}\")\n",
        "print(f\"Cleaned dataset: {OUT_CSV}\")\n",
        "print(f\"Audit file:      {AUDIT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGaBLWfYpF0e",
        "outputId": "50165086-e434-467e-cabc-f1690eb4e28c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning completed. Modified rows: 7444\n",
            "Cleaned dataset: /content/dataset_gbv_clean2.csv\n",
            "Audit file:      /content/gbv_twclean_audit.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Dedup \"truncated tweets\": texts ending with \"...\" or \"…\"\n",
        "# Consider duplicates if they are prefixes of longer versions.\n",
        "# Keep the longest version. Count duplicates overall, by CHANNEL, by EMOTION.\n",
        "# =========================================================\n",
        "IN_CSV  = \"/content/dataset_gbv_clean2.csv\"\n",
        "OUT_CSV = \"/content/dataset_gbv_clean3.csv\"\n",
        "DUP_CSV = \"/content/gbv_twclean_ellipsis_duplicates.csv\"\n",
        "\n",
        "ELLIPSIS_END_RE = re.compile(r\"(?:\\s*(?:\\.{3}|…))+$\")\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    \"\"\"Lightweight normalization to compare near-identical texts.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        s = \"\" if pd.isna(s) else str(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    s = s.casefold()\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def strip_trailing_ellipsis_norm(tnorm: str) -> Tuple[str, bool]:\n",
        "    \"\"\"Remove trailing ellipsis from normalized text; returns (prefix_norm, was_truncated?).\"\"\"\n",
        "    new = ELLIPSIS_END_RE.sub(\"\", tnorm)\n",
        "    return new, (new != tnorm)\n",
        "\n",
        "# 1) Load\n",
        "df = pd.read_csv(IN_CSV, sep=\";\", dtype=str, keep_default_na=False, na_values=[])\n",
        "df[\"CHANNEL\"] = df[\"CHANNEL\"].astype(str).str.strip().str.upper()\n",
        "df[\"EMOTION\"] = df[\"EMOTION\"].astype(str).str.strip().str.upper()\n",
        "\n",
        "# 2) Features for dedup\n",
        "df[\"_TNORM\"] = df[\"TEXT\"].map(normalize_text)\n",
        "df[\"_LEN\"]   = df[\"_TNORM\"].str.len()\n",
        "df[\"_PREFIX\"], df[\"_IS_TRUNC\"] = zip(*df[\"_TNORM\"].map(strip_trailing_ellipsis_norm))\n",
        "\n",
        "# 3) Build group key\n",
        "df[\"_GROUP_KEY\"] = df[\"_TNORM\"]\n",
        "tnorm_series = df[\"_TNORM\"]\n",
        "length_series = df[\"_LEN\"]\n",
        "trunc_idx = df.index[df[\"_IS_TRUNC\"]].tolist()\n",
        "\n",
        "for i in trunc_idx:\n",
        "    pref = df.at[i, \"_PREFIX\"]\n",
        "    if not pref:\n",
        "        continue\n",
        "    mask = tnorm_series.str.startswith(pref)\n",
        "    if not mask.any():\n",
        "        continue\n",
        "    cand = df.loc[mask, [\"_TNORM\",\"_LEN\"]].copy()\n",
        "    max_len = cand[\"_LEN\"].max()\n",
        "    best_tnorm = cand.loc[cand[\"_LEN\"] == max_len, \"_TNORM\"].iloc[0]\n",
        "    df.at[i, \"_GROUP_KEY\"] = best_tnorm\n",
        "\n",
        "# 4) Representative per group\n",
        "df[\"_ROW_ID\"] = range(len(df))\n",
        "rep_df = (\n",
        "    df.sort_values([\"_GROUP_KEY\",\"_LEN\",\"_ROW_ID\"], ascending=[True, False, True])\n",
        "      .groupby(\"_GROUP_KEY\", as_index=False)\n",
        "      .head(1)[[\"_GROUP_KEY\",\"_ROW_ID\"]]\n",
        "      .rename(columns={\"_ROW_ID\":\"_REP_ROW_ID\"})\n",
        ")\n",
        "df = df.merge(rep_df, on=\"_GROUP_KEY\", how=\"left\")\n",
        "duplicate_mask = df[\"_ROW_ID\"] != df[\"_REP_ROW_ID\"]\n",
        "\n",
        "df[\"_DUP_REASON\"] = pd.Series([\"exact\"] * len(df))\n",
        "df.loc[df[\"_GROUP_KEY\"] != df[\"_TNORM\"], \"_DUP_REASON\"] = \"ellipsis\"\n",
        "\n",
        "# 5) Counts\n",
        "total_removed = int(duplicate_mask.sum())\n",
        "by_channel = df.loc[duplicate_mask, \"CHANNEL\"].value_counts().sort_index()\n",
        "by_emotion = df.loc[duplicate_mask, \"EMOTION\"].value_counts().sort_index()\n",
        "\n",
        "print(\"=== DUPLICATES TO REMOVE (ELLIPSIS) ===\")\n",
        "print(f\"Total: {total_removed}\\n\")\n",
        "print(\"By CHANNEL:\")\n",
        "print(by_channel.to_string() if not by_channel.empty else \"(none)\", \"\\n\")\n",
        "print(\"By EMOTION:\")\n",
        "print(by_emotion.to_string() if not by_emotion.empty else \"(none)\")\n",
        "\n",
        "# 6) Save audit and final dataset\n",
        "dups = df.loc[duplicate_mask, [\"ID\",\"DATE\",\"CHANNEL\",\"EMOTION\",\"TEXT\",\"_DUP_REASON\",\"_GROUP_KEY\"]].copy()\n",
        "dups.rename(columns={\"TEXT\":\"TEXT_REMOVED\", \"_GROUP_KEY\":\"GROUP_KEY_TNORM\"}, inplace=True)\n",
        "\n",
        "rep_map = df.set_index(\"_ROW_ID\")[[\"TEXT\"]].to_dict()[\"TEXT\"]\n",
        "df[\"_REP_TEXT\"] = df[\"_REP_ROW_ID\"].map(rep_map)\n",
        "dups[\"TEXT_KEPT\"] = df.loc[dups.index, \"_REP_TEXT\"]\n",
        "\n",
        "df_nodup = df.loc[~duplicate_mask, [c for c in df.columns if not c.startswith(\"_\")]]\n",
        "df_nodup = df_nodup.drop(columns=[\"_REP_ROW_ID\",\"_ROW_ID\"], errors=\"ignore\")\n",
        "\n",
        "df_nodup.to_csv(OUT_CSV, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "dups.to_csv(DUP_CSV, sep=\";\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "print(\"Dedup (ellipsis) completed.\")\n",
        "print(\"Dataset without duplicates:\", OUT_CSV)\n",
        "print(\"Duplicates audit (removed vs kept):\", DUP_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37XE894pEPK",
        "outputId": "0992e982-599d-4156-b530-b54f5d1efd35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DUPLICATES TO REMOVE (ELLIPSIS) ===\n",
            "Total: 1506\n",
            "\n",
            "By CHANNEL:\n",
            "CHANNEL\n",
            "FACEBOOK     1111\n",
            "FEED            2\n",
            "INSTAGRAM     145\n",
            "TWITTER       248 \n",
            "\n",
            "By EMOTION:\n",
            "EMOTION\n",
            "GIOIA        194\n",
            "NEUTRA       592\n",
            "PAURA          9\n",
            "RABBIA       343\n",
            "SORPRESA      36\n",
            "TRISTEZZA    332\n",
            "Dedup (ellipsis) completed.\n",
            "Dataset without duplicates: /content/dataset_gbv_clean3.csv\n",
            "Duplicates audit (removed vs kept): /content/gbv_twclean_ellipsis_duplicates.csv\n"
          ]
        }
      ]
    }
  ]
}